{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohanraj/opt/anaconda3/lib/python3.7/site-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "/Users/rohanraj/opt/anaconda3/lib/python3.7/site-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n",
      "/Users/rohanraj/opt/anaconda3/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "/Users/rohanraj/opt/anaconda3/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "/Users/rohanraj/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "/Users/rohanraj/opt/anaconda3/lib/python3.7/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, Set, Iterable\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/kernels/scriptcontent/11511967/notebook\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import ast\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.mlab as mlab\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "# from textblob import TextBlob\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import tnt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import indian\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.summarization import summarize\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import stanfordnlp\n",
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# from bokeh.plotting import figure, output_file, show\n",
    "# from bokeh.models import Label\n",
    "# from bokeh.io import output_notebook\n",
    "# output_notebook()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/rohanraj/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_tokenizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/rohanraj/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_tagger.pt', 'pretrain_path': '/Users/rohanraj/stanfordnlp_resources/hi_hdtb_models/hi_hdtb.pretrain.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/rohanraj/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_lemmatizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/rohanraj/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_parser.pt', 'pretrain_path': '/Users/rohanraj/stanfordnlp_resources/hi_hdtb_models/hi_hdtb.pretrain.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline ( lang = 'hi' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = '../data/hindi.txt'\n",
    "with open(datafile,'r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    text = text.split(\"॥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1290/1290 [00:00<00:00, 427145.51it/s]\n"
     ]
    }
   ],
   "source": [
    "text_sentence = []\n",
    "i = 0\n",
    "for word in tqdm(text):\n",
    "#     print(i,word)\n",
    "    if re.findall(\"^\\s?\\d+\\s?\",word):\n",
    "        continue\n",
    "    else:   \n",
    "        text_sentence.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' वानर हनुमान सीमा के देखने की उत्कण्ठा येसूयास्त की प्रतीक्षा करता भया ',\n",
       " ' घ४-९) सीता को रावण के भन्तापुर में ढूंढना : वह महावीर्य महान हृदय पाछा वानर श्रेष्ठ रात के समय अद्वार से कोट को फांद कर का में प्रविष्ट हृया ']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sentence[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "667"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_sentence) #total sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hindi stopwords\n",
    "stop_words_df = pd.read_csv(\"../data/stopwords.txt\", header = None)\n",
    "stop_words = list(set(stop_words_df.values.reshape(1,-1).tolist()[0]))\n",
    "stop_words.extend([\"।\", \"।।\", \")\", \"(\", \",\",'\"',\"हे\", \"हो\", 'में','से','COMMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['सकती', 'बारे']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_str_from_list(original_read_text):\n",
    "    prepared_text = \"\"\n",
    "    for line in original_read_text:\n",
    "        line = line.split()\n",
    "#       print(line)\n",
    "        tmp_line = \" \".join(line)\n",
    "        prepared_text += \" \\n\"+tmp_line\n",
    "    return prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_text = create_str_from_list(text_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nवानर हनुमान सीमा के देखने की उत्कण्ठा येसूयास्त की प्रतीक्षा करता भया \\nघ४-९) सीता को रावण के भन्तापुर में ढूंढना : वह महावीर्य महान हृदय पाछा वानर श्रेष्ठ रात के समय अद्वार से कोट को फांद कर का में प्रविष्ट हृया \\nधानरराज के उस हितैषी ने लंका नगरी में प्रवेश करके मानों अपना बायां पाओं शत्रु के सिर पर रख दिया \\nउस समय सुन्दर \\'सब ओर से सजे हुए श्वेत मेघ के तुल्य राक्षसों के जो पद्माकार, स्वस्तिकाकार, और वर्धमान घर है, उन से इंका जगमन. कर रही थी, राधा के अर्थ वह श्रीमान् घूमता हुमा उसे देखता भया और आनन्दित होता भया ।। ३, ४ \\nएक भवन से दूसरे भवन को जाते हुए उस वानरश्रेष्ठ ने वहाँ र विविध भाकृति और रूपों वाले भवन देखे \\nवहाँ राक्षसों के घरों में उसने अप \"करते हुओं के मन्त्र मुने और स्वाध्याय में रत राक्षसों को देखा \\nराक्षसों के घर से घर और बगीचों को देखता हुआ बेधड़क यह महलों के पास घूमा \\nतव पवनपुत्र हनुमान ने यह भवन श्रेष्ठ देखा, जो राक्षसपति का भवन है, और बहुत महलों से भरपूर है \\nविशाल नेत्रोंवाली वैदेही सीता को ढूंढता हुआ शत्रुनों के मारनेवाला, हनुमान उसके चारों ओर घूमा ।। ९ \\nतब वह उस सुन्दर पड़ी माला की ओर मस्थित हुआ, जो उत्तम स्त्री की तरह रावण की बड़ी पारी थी \\nजिमकी सीदियों में माणेयां जड़ी हुई हैं, जो सोने के झरोकों से भूपित हैं, सङ्गमर्मर का फर्श है, और बीच २ में दान्त का काम किया हुआ है \\nजो सम, सीधे, बड़े कंचे पूरे २ मजे हुए स्तम्भों मे मानों आते ऊँचे पलों से छौ की ओर प्रस्थित हुई है \\nसोंचम गलीचा जिसमें पिछा हुआ है, राक्षों के अधिपति से सेवित है, मन को प्रसन्न करने वाली और शरीर की कान्ति को बढ़ाने वाली है।। १३ \\nदीपकों के प्रकाश से, रावण के तेज से, और भूषणों की चमक मे, पानों जलती हुई प्रतीत होती है। \\nउस शाला में देखते हुए इनुमान् ने रत्नों मे भाषेत एक दिव्य विलोरी शपनानन ( बैठने मोने का एला ) देखा \\nओर शराब पीकर बन्द हुए, श्रीद चमकते हुए पलङ्ग -. पर लेटे हुए राक्षसाधिपति को उस महारानर ने देखा \\nससके पास आकर बड़ा गीवन इा अत्यन्त डरे हुए की तरह पीछे-इट गया, और प्यारी खियों वाले उस राक्षसपति के घर. में बस वानर यूयपति ने चन्द्र तुल्य मुखवाली, सुन्दर कुण्डल पहने हुई, ताजे घुषषों की पालाएं और भूषणोंवाली पनियों को देसा \\nउन में से एकान्त स्थिव एक गुभ शय्या के ऊपर लेटी हुई रुपवती उम ने एक खो देखी \\nजो अपनी शोभा से मानों उस उत्तम भवन को शोभायमान करती थी, बद्द मन्दोदरी थी, जोकि सुन्दर रूपवती वहां लेटी हुई थी \\nमहाबाहु पवनसुत ने उस भूषित स्त्री को देखकर उमके रूप यौवन की सम्पदा से रूपाल किया, कि कदाचित यह सीता हो \\nय०१०-११) राधण के अन्तःपुर में सीता का न पाना पर उसी समय उस रूपाळ को पटाकर, स्थित हुा महा . घानर सीता के विषय भरा विचार करता भया \\nकि राम से वियुक्त हुई वह मुन्दरीन सो सक्ती है, न भोग रती है, न अलङ्कार कर सक्ती है, न पान सेवन कर सक्ती है \\nनि:सन्देह यह कोई और है, ऐसा'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_text[0:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"घ४-९) सीता को रावण के भन्तापुर में ढूंढना : वह महावीर्य महान हृदय पाछा वानर श्रेष्ठ रात के समय अद्वार से कोट को फांद कर का में प्रविष्ट हृया \\nउस समय सुन्दर 'सब ओर से सजे हुए श्वेत मेघ के तुल्य राक्षसों के जो पद्माकार, स्वस्तिकाकार, और वर्धमान घर है, उन से इंका जगमन.\\nराक्षसों के घर से घर और बगीचों को देखता हुआ बेधड़क यह महलों के पास घूमा \\nतब वह उस सुन्दर पड़ी माला की ओर मस्थित हुआ, जो उत्तम स्त्री की तरह रावण की बड़ी पारी थी \\nमें बस वानर यूयपति ने चन्द्र तुल्य मुखवाली, सुन्दर कुण्डल पहने हुई, ताजे घुषषों की पालाएं और भूषणोंवाली पनियों को देसा \""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(str_text[0:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68718"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(str_text):\n",
    "    startflag = 0\n",
    "    endflag = 2500\n",
    "    parsed_text = {'text':[], 'title':[]}\n",
    "    for summary in range(int(len(str_text)/endflag)):\n",
    "        summarization_block = summarize(str_text[startflag:endflag])\n",
    "        parsed_text['text'].append(str_text[startflag:endflag])\n",
    "        parsed_text['title'].append(summarization_block)\n",
    "        startflag +=2500\n",
    "        endflag +=2500\n",
    "    return pd.DataFrame(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = summarization(str_text)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('input_abstarct_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Summarization from Gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrankTfIdf(document):\n",
    "    # sentence_tokenizer = PunktSentenceTokenizer()\n",
    "    # sentences = sentence_tokenizer.tokenize(document, 'hindi')\n",
    "\n",
    "    sentences = document\n",
    "    bow_matrix = CountVectorizer().fit_transform(sentences)\n",
    "    # normalized = TfidfTransformer(norm='l2', use_idf=True, use_bm25idf=True, smooth_idf=True,\n",
    "    #              delta_idf=False, sublinear_tf=False, bm25_tf=True).fit_transform(bow_matrix)\n",
    "\n",
    "    normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    "    similarity_graph = normalized * normalized.T\n",
    "\n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    return sorted(((scores[i], s) for i, s in enumerate(sentences)),\n",
    "                  reverse=True)\n",
    "def orderSentences(rankedList, data, initSentences):\n",
    "    index = ['']*len(data)\n",
    "    # print(rankedList)\n",
    "    for eachRanked in rankedList[0:int(math.ceil(0.2*len(rankedList)))]:\n",
    "        sen = eachRanked[1]\n",
    "        index[data.index(sen)] = initSentences[data.index(sen)]\n",
    "        # print(data.index(sen))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(text_sentence)/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_summarization(text_sentence):\n",
    "    startflag = 0\n",
    "    endflag = 25\n",
    "    parsed_text = {'text':[], 'alt_title':[]}\n",
    "    sentence =''\n",
    "    for summary in range(int(len(text_sentence)/endflag)):\n",
    "        rankedSentences = textrankTfIdf(text_sentence[startflag:endflag])\n",
    "        orderedsentences = orderSentences(rankedSentences, text_sentence[startflag:endflag], text_sentence[startflag:endflag])\n",
    "        for ordered in orderedsentences:\n",
    "            if ordered != \"\":\n",
    "                sentence += ordered \n",
    "        parsed_text['alt_title'].append(sentence)\n",
    "        parsed_text['text'].append(text_sentence[startflag:endflag])\n",
    "        startflag +=25\n",
    "        endflag +=25\n",
    "        sentence =''\n",
    "    return pd.DataFrame(parsed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = alternate_summarization(text_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>alt_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ वानर हनुमान सीमा के देखने की उत्कण्ठा येसूया...</td>\n",
       "      <td>उस समय सुन्दर 'सब ओर से सजे हुए श्वेत मेघ के ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ मेरी दृष्टि आजतक (ऐमी अवस्था में ) परस्त्रिय...</td>\n",
       "      <td>सो मैंने शुद्ध मन से रावण का पारा अन्तःपुर ढू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ तब उस वानर ने सारे जगह में सुहावना एक पर्वत ...</td>\n",
       "      <td>तब उस वानर ने सारे जगह में सुहावना एक पर्वत द...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ , १८) प्रभात का समय और रावण का अशोकवनिका में...</td>\n",
       "      <td>उस समय मङ्गक बाओं और कानों के प्यारे शब्दों स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ पह तेरा सुन्दर बना हुथा, यौवन चला जारहा है, ...</td>\n",
       "      <td>सारे लोकों से पल से हर कर जो मैं रत्न लाचा हू...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [ वानर हनुमान सीमा के देखने की उत्कण्ठा येसूया...   \n",
       "1  [ मेरी दृष्टि आजतक (ऐमी अवस्था में ) परस्त्रिय...   \n",
       "2  [ तब उस वानर ने सारे जगह में सुहावना एक पर्वत ...   \n",
       "3  [ , १८) प्रभात का समय और रावण का अशोकवनिका में...   \n",
       "4  [ पह तेरा सुन्दर बना हुथा, यौवन चला जारहा है, ...   \n",
       "\n",
       "                                           alt_title  \n",
       "0   उस समय सुन्दर 'सब ओर से सजे हुए श्वेत मेघ के ...  \n",
       "1   सो मैंने शुद्ध मन से रावण का पारा अन्तःपुर ढू...  \n",
       "2   तब उस वानर ने सारे जगह में सुहावना एक पर्वत द...  \n",
       "3   उस समय मङ्गक बाओं और कानों के प्यारे शब्दों स...  \n",
       "4   सारे लोकों से पल से हर कर जो मैं रत्न लाचा हू...  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('summary_alt_gensim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../data/summary_alt_gensim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ner(sentence_ner):\n",
    "    parsed_text = {'word':[], 'ner':[]}\n",
    "    words = re.findall(\"([^<]+)<[^>]+>\",sentence_ner)\n",
    "    tags = re.findall(\"<([^>]+)>\",sentence_ner)\n",
    "    \n",
    "    # for word in words: append word #we need to write seperate tags\n",
    "    \n",
    "    for index in range(len(words)):\n",
    "        parsed_text['word'].append(words[index])\n",
    "        parsed_text['ner'].append(tags[index])\n",
    "    return pd.DataFrame(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-03 19:40:57,907 loading file /Users/rohanraj/Documents/Master/CaseStudy2/code/resources/taggers/netag/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# load the model you trained\n",
    "model = SequenceTagger.load('/Users/rohanraj/Documents/Master/CaseStudy2/code/resources/taggers/netag/best-model.pt')\n",
    "\n",
    "df_ner = pd.DataFrame()\n",
    "\n",
    "r, c = df2.shape\n",
    "for row_no in range(r):\n",
    "    sentence = Sentence(df2['alt_title'][row_no])\n",
    "    model.predict(sentence)\n",
    "    sentence_ner = sentence.to_tagged_string()\n",
    "    temp_df = extract_ner(sentence_ner)\n",
    "    df_ner = df_ner.append(temp_df,ignore_index=True)  \n",
    "# predict tags and print\n",
    "#print(sentence.to_tagged_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner.to_csv('../data/ner_tag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ner = sentence.to_tagged_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= re.findall(\"([^<]+)<[^>]+>\",sentence_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = re.findall(\"([^<]+)<[^>]+>\",sentence_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-LOCATION',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-COUNT',\n",
       " 'B-COUNT',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-LOCATION',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'I-LOCATION',\n",
       " 'I-LOCATION',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-LOCATION',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-LOCATION',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-LOCATION',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-LOCATION',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'I-LOCATION',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-LOCATION',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'B-PERSON',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"<([^>]+)>\",sentence_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<B-PERSON>',\n",
       " '<B-LOCATION>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-COUNT>',\n",
       " '<B-COUNT>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-LOCATION>',\n",
       " '<B-PERSON>',\n",
       " '<I-LOCATION>',\n",
       " '<I-LOCATION>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-LOCATION>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-LOCATION>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-LOCATION>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-LOCATION>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<I-LOCATION>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-LOCATION>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>',\n",
       " '<B-PERSON>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re.findall(\"<[A-Z]-[A-Z]*>\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>उस</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>समय</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>सुन्दर</td>\n",
       "      <td>B-PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'सब</td>\n",
       "      <td>B-LOCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ओर</td>\n",
       "      <td>B-PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word         ner\n",
       "0       उस            o\n",
       "1      समय            o\n",
       "2   सुन्दर     B-PERSON\n",
       "3      'सब   B-LOCATION\n",
       "4       ओर     B-PERSON"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ner_df = extract_ner(sentence)\n",
    "extract_ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Token: 1 उस, Token: 2 समय, Token: 3 सुन्दर, Token: 4 'सब, Token: 5 ओर, Token: 6 से, Token: 7 सजे, Token: 8 हुए, Token: 9 श्वेत, Token: 10 मेघ, Token: 11 के, Token: 12 तुल्य, Token: 13 राक्षसों, Token: 14 के, Token: 15 जो, Token: 16 पद्माकार,, Token: 17 स्वस्तिकाकार,, Token: 18 और, Token: 19 वर्धमान, Token: 20 घर, Token: 21 है,, Token: 22 उन, Token: 23 से, Token: 24 इंका, Token: 25 जगमन., Token: 26 कर, Token: 27 रही, Token: 28 थी,, Token: 29 राधा, Token: 30 के, Token: 31 अर्थ, Token: 32 वह, Token: 33 श्रीमान्, Token: 34 घूमता, Token: 35 हुमा, Token: 36 उसे, Token: 37 देखता, Token: 38 भया, Token: 39 और, Token: 40 आनन्दित, Token: 41 होता, Token: 42 भया, Token: 43 ।।, Token: 44 ३,, Token: 45 ४, Token: 46 वहाँ, Token: 47 राक्षसों, Token: 48 के, Token: 49 घरों, Token: 50 में, Token: 51 उसने, Token: 52 अप, Token: 53 \"करते, Token: 54 हुओं, Token: 55 के, Token: 56 मन्त्र, Token: 57 मुने, Token: 58 और, Token: 59 स्वाध्याय, Token: 60 में, Token: 61 रत, Token: 62 राक्षसों, Token: 63 को, Token: 64 देखा, Token: 65 तब, Token: 66 वह, Token: 67 उस, Token: 68 सुन्दर, Token: 69 पड़ी, Token: 70 माला, Token: 71 की, Token: 72 ओर, Token: 73 मस्थित, Token: 74 हुआ,, Token: 75 जो, Token: 76 उत्तम, Token: 77 स्त्री, Token: 78 की, Token: 79 तरह, Token: 80 रावण, Token: 81 की, Token: 82 बड़ी, Token: 83 पारी, Token: 84 थी, Token: 85 ओर, Token: 86 शराब, Token: 87 पीकर, Token: 88 बन्द, Token: 89 हुए,, Token: 90 श्रीद, Token: 91 चमकते, Token: 92 हुए, Token: 93 पलङ्ग, Token: 94 -., Token: 95 पर, Token: 96 लेटे, Token: 97 हुए, Token: 98 राक्षसाधिपति, Token: 99 को, Token: 100 उस, Token: 101 महारानर, Token: 102 ने, Token: 103 देखा, Token: 104 जो, Token: 105 अपनी, Token: 106 शोभा, Token: 107 से, Token: 108 मानों, Token: 109 उस, Token: 110 उत्तम, Token: 111 भवन, Token: 112 को, Token: 113 शोभायमान, Token: 114 करती, Token: 115 थी,, Token: 116 बद्द, Token: 117 मन्दोदरी, Token: 118 थी,, Token: 119 जोकि, Token: 120 सुन्दर, Token: 121 रूपवती, Token: 122 वहां, Token: 123 लेटी, Token: 124 हुई, Token: 125 थी]\n"
     ]
    }
   ],
   "source": [
    "print(sentence.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-01 20:29:04,306 loading file /Users/rohanraj/Documents/Master/CaseStudy2/code/resources/taggers/netag/best-model.pt\n",
      "घ४-९) <o> सीता <B-PERSON> को <B-PERSON> रावण <B-PERSON> के <B-PERSON> भन्तापुर <B-LOCATION> में <B-PERSON> ढूंढना <o> : <o> वह <B-PERSON> महावीर्य <B-PERSON> महान <B-PERSON> हृदय <B-PERSON> पाछा <B-PERSON> वानर <B-PERSON> श्रेष्ठ <B-PERSON> रात <B-PERSON> के <o> समय <o> अद्वार <B-PERSON> से <o> कोट <B-PERSON> को <o> फांद <o> कर <o> का <o> में <o> प्रविष्ट <o> हृया <o> \n",
      "उस <o> समय <o> सुन्दर <o> 'सब <B-PERSON> ओर <o> से <o> सजे <o> हुए <o> श्वेत <B-PERSON> मेघ <o> के <o> तुल्य <o> राक्षसों <o> के <o> जो <o> पद्माकार, <o> स्वस्तिकाकार, <o> और <o> वर्धमान <o> घर <o> है, <o> उन <o> से <o> इंका <o> जगमन.\n",
      "राक्षसों <B-PERSON> के <o> घर <o> से <o> घर <o> और <o> बगीचों <I-LOCATION> को <o> देखता <o> हुआ <o> बेधड़क <I-LOCATION> यह <o> महलों <I-LOCATION> के <o> पास <o> घूमा <o> \n",
      "तब <o> वह <o> उस <o> सुन्दर <B-PERSON> पड़ी <o> माला <I-LOCATION> की <I-LOCATION> ओर <o> मस्थित <B-PERSON> हुआ, <o> जो <B-PERSON> उत्तम <B-LOCATION> स्त्री <B-PERSON> की <B-PERSON> तरह <o> रावण <B-PERSON> की <B-PERSON> बड़ी <o> पारी <o> थी <o> \n",
      "में <o> बस <o> वानर <B-PERSON> यूयपति <B-LOCATION> ने <B-PERSON> चन्द्र <B-PERSON> तुल्य <B-LOCATION> मुखवाली, <B-PERSON> सुन्दर <B-PERSON> कुण्डल <o> पहने <o> हुई, <o> ताजे <B-LOCATION> घुषषों <B-PERSON> की <B-LOCATION> पालाएं <B-PERSON> और <o> भूषणोंवाली <B-PERSON> पनियों <o> को <I-LOCATION> देसा <B-LOCATION>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# load the model you trained\n",
    "model = SequenceTagger.load('/Users/rohanraj/Documents/Master/CaseStudy2/code/resources/taggers/netag/best-model.pt')\n",
    "\n",
    "# create example sentence\n",
    "#sentence = Sentence('मोहनदास करमचन्द गांधी (२ अक्टूबर १८६९ - ३० जनवरी १९४८) भारत एवं भारतीय स्वतंत्रता आंदोलन के एक प्रमुख राजनैतिक एवं आध्यात्मिक नेता थे।')\n",
    "sentence = Sentence(df['title'][0])\n",
    "# predict tags and print\n",
    "model.predict(sentence)\n",
    "\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standford nlp is being used for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    lemmatized_text = []\n",
    "    for line in tqdm(text):\n",
    "        if line not in [\"\",\" \"] :\n",
    "            doc = nlp(line)\n",
    "            for sent in doc.sentences:\n",
    "                for wrd in sent.words:\n",
    "                    #extract text and lemma\n",
    "                    lemmatized_text.append(wrd.lemma)\n",
    "    return lemmatized_text\n",
    "\n",
    "def remove_stopwords(word_tokenized,stop_words):\n",
    "    return [word for word in word_tokenized if word not in stop_words]\n",
    "\n",
    "def custom_remove_garbage(original_words_list,list_of_garbage_words):\n",
    "    tmp_list = [word for word in original_words_list if word not in list_of_garbage_words] # garbage list\n",
    "    tmp_list = [word for word in tmp_list if len(re.findall(\"\\d+\",word))==0] # english numbers\n",
    "    tmp_list = [word for word in tmp_list if len(re.findall(\"[a-zA-Z]+\",word))==0] # english alphabets\n",
    "    return tmp_list\n",
    "\n",
    "def Diff(li1, li2): \n",
    "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2] \n",
    "    return li_dif "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Meddling after prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modelling(text_sentence,stop_words):\n",
    "    df = pd.DataFrame()\n",
    "    startflag = 0\n",
    "    endflag = 25\n",
    "    for text in range(int(len(text_sentence)/endflag)):\n",
    "        \n",
    "        #Clean text after lemmatization\n",
    "        lemmatized = lemmatization(text_sentence[startflag:endflag])\n",
    "        clean_text = remove_stopwords(lemmatized,stop_words)\n",
    "        #len(clean_text),len(lemmatized) #281,612\n",
    "        \n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in clean_text]\n",
    "        #print(stripped[:25])\n",
    "        #print(len(clean_text),len(stripped)) #281\n",
    "\n",
    "        resultant = Diff(clean_text,stripped)\n",
    "        #len(set(resultant)) #19\n",
    "\n",
    "        final_text = custom_remove_garbage(stripped,resultant)\n",
    "        #len(final_text) #265\n",
    "        \n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary([final_text])\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = final_text\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(texts)] \n",
    "        \n",
    "        # Build LDA model\n",
    "\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=10,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        #pprint(lda_model.print_topics())\n",
    "        temp_file = datapath(\"model\")\n",
    "        lda_model.save(temp_file)\n",
    "        lda = gensim.models.ldamodel.LdaModel.load(temp_file)\n",
    "        lda.update(corpus)\n",
    "        lda.save('model')\n",
    "        temp_df = pd.DataFrame(lda.print_topics())\n",
    "        df = df.append(temp_df,ignore_index=True) \n",
    "        startflag +=25\n",
    "        endflag +=25\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 10.40it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.90it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 11.24it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.24it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.86it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.41it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.38it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.67it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.58it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.50it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.82it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.73it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  8.61it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.98it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.49it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.90it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.69it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.69it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.37it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.61it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  8.37it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.89it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00, 10.21it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.29it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.22it/s]\n",
      "100%|██████████| 25/25 [00:02<00:00,  9.37it/s]\n"
     ]
    }
   ],
   "source": [
    "df4 = topic_modelling(text_sentence,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 2)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "      <td>0.005*\"सेना\" + 0.005*\"सारण\" + 0.005*\"कह\" + 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>0.005*\"सेना\" + 0.005*\"सारण\" + 0.005*\"कह\" + 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2</td>\n",
       "      <td>0.005*\"सेना\" + 0.005*\"सारण\" + 0.005*\"कह\" + 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3</td>\n",
       "      <td>0.005*\"सेना\" + 0.005*\"कह\" + 0.005*\"सारण\" + 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4</td>\n",
       "      <td>0.036*\"सेना\" + 0.029*\"सारण\" + 0.025*\"कह\" + 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                                                  1\n",
       "125  0  0.005*\"सेना\" + 0.005*\"सारण\" + 0.005*\"कह\" + 0.0...\n",
       "126  1  0.005*\"सेना\" + 0.005*\"सारण\" + 0.005*\"कह\" + 0.0...\n",
       "127  2  0.005*\"सेना\" + 0.005*\"सारण\" + 0.005*\"कह\" + 0.0...\n",
       "128  3  0.005*\"सेना\" + 0.005*\"कह\" + 0.005*\"सारण\" + 0.0...\n",
       "129  4  0.036*\"सेना\" + 0.029*\"सारण\" + 0.025*\"कह\" + 0.0..."
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv('combined_model_topic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = datapath(\"model\")\n",
    "lda = gensim.models.ldamodel.LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
